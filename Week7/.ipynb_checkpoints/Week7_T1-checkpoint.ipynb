{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9fabe",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert Week7_T1.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9951af",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961b96d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural networks\n",
    "\n",
    "* Wat zijn dat?\n",
    "* Wat kunnen ze wat andere leer modellen niet kunnen?\n",
    "* Hoe zien ze eruit? Hoe trainen we ze?\n",
    "\n",
    "## Beide taken\n",
    "\n",
    "* Zowel **regressie** (numerieke waarde voorspellen) als **classificatie** (klasse voorspellen)\n",
    "* Heel simpel: net als bij lineare regressie: gebruik de logistic/sigmoid functie om van elke output een getal tussen 0 en 1 (een kans dus) te maken.\n",
    "    * Voor multi-class classificatie: bereken de kans per klasse, en neem de argmax. (meestal `softmax` genoemd)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9cece",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural network\n",
    "\n",
    "* Een (feedforward) neuraal netwerk is een **gewogen gericht acyclisch netwerk**: \n",
    "    * knopen, \n",
    "    * pijlen,\n",
    "    * gewichten op de pijlen\n",
    "* Drie soorten knopen\n",
    "    * Inputs\n",
    "    * Output\n",
    "    * Hidden\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88df94e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# simplest neural network\n",
    "\n",
    "![](../img/nn/onenodeperceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182f323",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hey but don't we know that already?\n",
    "\n",
    "![](../img/nn/onenodeperceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1a82c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hey but don't we know that already? Sure logistic regression! \n",
    "\n",
    "\n",
    "$$y = \\sigma(w_0 + w_1x_1 + w_2x_2 + w_3x_3)$$\n",
    "* With $\\sigma(x)$ the sigmoid/logistic (activation) function. \n",
    "![](../img/nn/onenodeperceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07bbb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Don't be dissappointed\n",
    "\n",
    "* A neural network is nothing more than a bunch of one node perceptrons stacked next to and after each other.\n",
    "\n",
    "### two directions\n",
    "\n",
    "1. More nodes inside one layer\n",
    "2. More layers.\n",
    "\n",
    "### Two architectures are \"spurious\"\n",
    "\n",
    "* one hidden layer with several nodes,  all fully connected to inoput and output\n",
    "* \"kralenketting\": several layers, all with one node.\n",
    "\n",
    "### so the simplest real one is then....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd698ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your first real neural network\n",
    "\n",
    "![](../img/nn/2layarNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28b5dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your first real neural network with the equations and the loss\n",
    "\n",
    "* Note that we added bias as well to the second layer\n",
    "* For the loss: recall that $y\\in \\{0,1\\}$, we do **binary cllassification**.\n",
    "\n",
    "![](../img/nn/2layarNN2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91c479",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nog even die logloss functie\n",
    "\n",
    "$$ -y\\log \\hat y - (1-y)\\log (1-\\hat y )$$\n",
    "\n",
    "* als $y=1$ dan loss = $-log \\hat y$\n",
    "* als $y=0$, dan loss = $-log (1- \\hat y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef4256b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        ,  0.69314718,  1.09861229,  1.38629436,  1.60943791,\n",
       "        1.79175947,  1.94591015,  2.07944154,  2.19722458])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# als y=1, wil je dat y-hat hoog zit.\n",
    "ys= [1/x for  x in range(1,10)]\n",
    "\n",
    "-1*np.log(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9e2668",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tycho\\AppData\\Local\\Temp\\ipykernel_31988\\57292759.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  -1*np.log(1-ys)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([       inf, 0.69314718, 0.40546511, 0.28768207, 0.22314355,\n",
       "       0.18232156, 0.15415068, 0.13353139, 0.11778304])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# als y=0, wil je dat y-hat juist laag zit.\n",
    "\n",
    "ys= np.array([1/x for  x in range(1,10)])\n",
    "\n",
    "-1*np.log(1-ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b5525",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now, lets train it! \n",
    "\n",
    "* Remember, what does training entail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1d015",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Finding those weights and biases which **reduce the loss**\n",
    "    * How many do we have? And where are they in the picture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438b6c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How do we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960b5bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* With gradient descent\n",
    "    * and of course the **partial derivatives** of each weight and bias with respect to the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1f251",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Partial derivatives\n",
    "\n",
    "* Can all be calculated using\n",
    "    * chain rule\n",
    "    * basic derivative calculus rules.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a4bc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Ketting regel](https://www.google.com/search?q=ketting+regel)\n",
    "\n",
    ">Hoe werkt de kettingregel? De kettingregel werkt als volgt: als h(x) = f(g(x)) dan is h'(x) = f'(g(x))*g'(x). Je neemt dus eerst de afgeleide van de buitenste functie en hierbij laat je de binnenste functie staan. Vervolgens vermenigvuldig je dit met de afgeleide van de binnenste functie.\n",
    "\n",
    "$$  h(x) = f(g(x))  \\Rightarrow  h'(x) = f'(g(x))\\cdot g'(x)$$\n",
    "\n",
    "#### Dus je kunt de afgeleide van $g(x)$ hergebruiken. bij het bepalenn van de afgeleide van $ h(x) = f(g(x)) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e1706",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Vb $h(x) = f(g(x))= (3 - 6x^2)^5$\n",
    "\n",
    "* $f(x)= x^5$\n",
    "* $g(x)=3 - 6x^2$.\n",
    "* Dus $f'(x)= 5x^4$ en $g'(x)=- 2\\cdot 6x^1= -12x$\n",
    "\n",
    "\n",
    "### dus\n",
    "\n",
    "$$h'(x) =  f'(g(x))\\cdot g'(x). = 5\\cdot (3 - 6x^2)^4\\cdot -12x = -60x\\cdot (3 - 6x^2)^4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdac43e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back propagation\n",
    "\n",
    "* The more layers we have, we more complex the expressions of the partial derivatives \n",
    "    * the more to the beginning, the \"longer the chain rule\"\n",
    "* but, if you calculate those partial derivatives from right to left (from easy to hard)\n",
    "    * you can **reuse** earlier partial derivatives\n",
    "        * because the chain rule is a product of partial derivatives\n",
    "* Storing those already calculated partial derivatives and reusing them, \"when you walk back (towrds the input layer)\", is called\n",
    "    * back propagation.\n",
    "* Illustration and example from [Coursera course Calculus for ML and DS](https://www.coursera.org/learn/machine-learning-calculus/lecture/51n2T/gradient-descent-and-backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9db41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back propagation, notation\n",
    "\n",
    "![](../img/nn/bp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff80d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back propagation, reuse\n",
    "\n",
    "![](../img/nn/bp1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e551e39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back propagation, reuse\n",
    "\n",
    "![](../img/nn/bp2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce9418",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Voorbeeld MLP\n",
    "\n",
    "* Uit het boek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d91553",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mglearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmglearn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_moons\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mglearn'"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70d990",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  MLP 10 layers van elk 10 knopen\n",
    "\n",
    "```\n",
    "MLPClassifier(solver='lbfgs', \n",
    "              activation='tanh', \n",
    "              max_iter=1000,\n",
    "              random_state=0, \n",
    "              hidden_layer_sizes=[10, 10],\n",
    "              alpha=.5)\n",
    "```\n",
    "\n",
    "* `alpha=.5` is de l2 regularisatie, net als bij regressie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b504bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "# using two hidden layers, with 10 units each, now with tanh nonlinearity\n",
    "mlp = MLPClassifier(solver='lbfgs', activation='tanh', max_iter=1000,\n",
    "                    random_state=0, hidden_layer_sizes=[10, 10], alpha=.5)\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3);\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train);\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19cf41d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wat is er nou anders dan bij lineare regressie?\n",
    "\n",
    "* Dit is duidelijk een niet lineaire scheider.\n",
    "* Dat komt door die **niet lineaire activatie functies**\n",
    "    * sigmoid\n",
    "    * tanh\n",
    "    * relu\n",
    "    \n",
    "## tanh\n",
    "\n",
    "* net alls sigmoid, maar range is (-1,1)\n",
    "\n",
    "## relu\n",
    "\n",
    "* Maakt alle waardes groter of gelijk aan 0\n",
    "    * als waarde positief is blijft het hetze;fde\n",
    "    * als het negatief is, wordt het 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff1ab6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning\n",
    "\n",
    ">As you probably have realized by now, there are many ways to control the complexity of a neural network: the number of hidden layers, the number of units in each hidden layer, and the regularization (alpha). There are actually even more, which we won’t go into here. \n",
    "\n",
    "## Let op\n",
    "\n",
    "* gewichten worden elke keer random ingesteld\n",
    "* dus het getrainede netwerk kan elke keer (ietsje) anders zijn.\n",
    "* hoe veel hangt af van de kwaliteit en hoeveelheid van je data, en de complexiteit van je model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d92c73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complexiteit van het model\n",
    "\n",
    "* Wordt meestal gegeven in het aantal parameters (= gewichten + biastermen) dat geleerd moet worden.\n",
    "\n",
    "### visueel\n",
    "\n",
    "* aantal gewichten = aantal pijlen\n",
    "* bias wordt vaak vergeten in de plaatjes,\n",
    "    * zo ja, dan moet je er dus voor elke laag 1 parameter bij optellen.\n",
    "    \n",
    "### Wat is veel?\n",
    "\n",
    "* ChatGPT heet 20 miljard parameters. GPT3 heeft er 175 miljard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc66a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scaling\n",
    "\n",
    "> Neural networks also expect all input features to vary in a similar way, and ideally to have a mean of 0, and a variance of 1. \n",
    "\n",
    "\n",
    "Gaat heel makkelijk met sklearn's StandardScalar.\n",
    "\n",
    "### Hoe kunnen we het zelf doen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfef76",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* trek voor elke waarde in elke kolom het gemiddelde van die kolom eraf.\n",
    "    * krijg je mean van 0\n",
    "* en deel door de std van die kolom\n",
    "    * std van 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a225676c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning \n",
    "\n",
    "```\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format( mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n",
    "```\n",
    "\n",
    "## output\n",
    "\n",
    "```\n",
    "Out[100]:\n",
    "    Accuracy on training set: 0.995\n",
    "    Accuracy on test set: 0.965\n",
    "```\n",
    "\n",
    "\n",
    "### How do you call this situation and what can we do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40cbd2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Overfitting, and we may overcome it by more regularaisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a897bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimizers\n",
    "\n",
    "* **How** to learn a model?\n",
    "\n",
    "#### Hordes waar je tegenaan loopt\n",
    "\n",
    "* Vaak is de data te enorm om gradient descent/backpropagation zo toe te passen als \"in theorie\" (duurt veel te lang)\n",
    "* Data is heterogeen (ook nog na herschalen)\n",
    "* learning rate is lastig in te stellen en zou misschie ook niet alktijd hetzelfde moeten zij\n",
    "    * eerst grote stappenn, later veel kleiner\n",
    "    * niet voor elke feature gelijk\n",
    "    \n",
    "#### Wat te doen, wat te kiezen?\n",
    "\n",
    "* Eigenlijk te vergevorderd voor deze cursus\n",
    "* Volg goede voorbeelden\n",
    "    * `adam` : Adaptive Moment Estimation (Diederik Kingma, eerste cum laude promovendus hier aan UvA Informatica sinds tientallen jaren)\n",
    "    * `lbfgs` : limited memory BFGS (namen van 4 mensen)\n",
    "    * `sgd` : stochastic gradient descent\n",
    "        * neem steeds steekproeven waarop je de gradiennt berekend, dus niet over alle data\n",
    "        * zijn ook weer heel veel mogelijkheden/varianten\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34282f6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap up AML course\n",
    "\n",
    "### 3 types of learning\n",
    "\n",
    "* Unsupervised\n",
    "    * clustering\n",
    "    * dimensionality reduction \n",
    "* Supervised\n",
    "    * regression\n",
    "    * classification\n",
    "* Reinforcement learning (not covered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f747a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap up AML course\n",
    "\n",
    "###  How to learn?\n",
    "\n",
    "1. Just remember all past examples: KNN\n",
    "2. Parametric\n",
    "    * Neem maar aan dat de data een bepaalde \"vorm\" heeft, en \n",
    "    * gebruik een leertechniek die daar goed bij aan sluit\n",
    "        * linear models: lin regression, SVM\n",
    "        * non linear models: Neural networks\n",
    "3. Non parametric: \n",
    "    * maak geen enkele aanname\n",
    "        * decision trees\n",
    "4. Ensemble learning\n",
    "    * soort wisdom of the crowd\n",
    "    * random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520174b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap up AML course\n",
    "\n",
    "### Focus on *evaluation*\n",
    "\n",
    "* ingebakken in het hele idee van ML\n",
    "* train-validatie-test\n",
    "* Ook bij unsupervised learning!\n",
    "* Heerlijke manier van werken:\n",
    "    * je hoeft iemand niet met een verhaal te overtuigen\n",
    "    * maar je hebt de standaard maten en de data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46112a24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap up AML course\n",
    "\n",
    "### Techniek\n",
    "\n",
    "* `sklearn` kan al heel erg veel, en heel erg handig en efficient.\n",
    "* als je echt NN wilt gaan doen, andere libraries.\n",
    "\n",
    "### Data,.......\n",
    "\n",
    "* een groot deel van de \"kunst\" van ML zit em in \n",
    "    * data preprocessing, feature engineering\n",
    "    * het **voorbereidende werk** (tot wel 80% van al het werk)\n",
    "* daar hebben we hier niet veel aandacht aan besteed.\n",
    "* meer in de Data Science cursus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765df58d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The book: what did we do?\n",
    "\n",
    "* *In fact, quite a lot!* \n",
    "\n",
    "1. Chap 1: introduction $\\checkmark$\n",
    "2. Chap 2: Supervised learning: all except 2.4 $\\checkmark$\n",
    "3. Chap 3: Unsupervised Learning and Preprocessing: $\\checkmark$\n",
    "    * preprocessing, most\n",
    "    * dimensionality reduction: PCA (NMF you did inn CI),\n",
    "    * clustering: most \n",
    "4. Chap 4: Representing Data and Engineering Features\n",
    "    * only 4.5 PolynomialFeatures $\\checkmark$\n",
    "    * Much more in your DS course\n",
    "5. Chap 5: Model Evaluation and Improvement $\\checkmark$\n",
    "6. Chap 6: Pipelines. NONE\n",
    "    * Handy! Invest in it if you will do ML seriously.\n",
    "    * Engineering.\n",
    "7. Chap 7: working with Text: NONE.\n",
    "    * But some done in CI\n",
    "        * TF-IDF count vectors\n",
    "        * cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c33f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tips voor tentamen\n",
    "\n",
    "* Ga door de opgaven. Bekijk ook oplossingen van anderen.\n",
    "* Doe boek/internet/voorbeeldopgave dicht, en maak een opgave uit je hoofd/met een notebook.\n",
    "* Idem voor de slides.\n",
    "* Bestudeer de opgegeven stukken van het boek goed.\n",
    "    * Je hebt het boek op het tentamen als PDF.\n",
    "    * Je krijgt dus geen \"weet-vraagjes\", maar inzicht vragen.\n",
    "    * Het is dus onhandig om weetjes uit je hoofd te gaan leren.\n",
    "        * Maar wel handig als je ze snel weet te vinden in het boek.\n",
    "* Wordt echt handig met je Jupyter notebook:\n",
    "    * puntje TAB\n",
    "    * vraagteken\n",
    "    * autocompletion\n",
    "* Snap de hints gegeven in de `assert` tests.\n",
    "    * juiste datatype\n",
    "    * valt in een bepaalde range (als gevraagd wordt om een kans, geef je geen percentage)\n",
    "* Schrijf en herschrijf en herschrijf je spiekbrief. \n",
    "    * Eerst gewoon maar lekker groot\n",
    "    \n",
    "### Heel veel succes!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ceddba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bedankt! \n",
    "\n",
    "* Studenten, heel erg bedankt voor de leuke cursus!\n",
    "* Misschien zie ik sommigen van jullie volgend jaar wel bij een scriptie."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
