{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27f7785",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "privacy",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment Applied Machine Learning BSc IK \n",
    "\n",
    "## Notebook made by\n",
    "\n",
    "**Gebruik graag dit formaat**\n",
    "\n",
    "* Voor de namen:  voornaam rest van je naam, voornaam rest van je naam,....\n",
    "* je studentnummers: hetzelfde: scheidt met `,`\n",
    "* je emails: hetzelfde: scheidt met `,`\n",
    "* voor je groep: **alleen de hoofdletter** (iets als  `A` of `B` dus)\n",
    "\n",
    "__Namen__: Tycho Stam, Lars de Jong\n",
    "    \n",
    "__Emails__: tycho.stam@student.uva.nl, 2002larsdejong@gmail.com\n",
    "\n",
    "__Student ids__ : 13303147, 13978268\n",
    "\n",
    "__Groep__ : B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ec59d",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* Een aantal opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. Dit zijn dus *NIET* alle tests, die komen er bij het graden nog bij.\n",
    "* Elke vraag is 1 punt waard, tenzij anders aangegeven. Soms is die punt onderverdeeld in deelpunten, maar niet altijd. \n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren\n",
    "\n",
    "## Na het inleveren!\n",
    "\n",
    "* Het gebeurt erg vaak dat mensen een \"leeg bestand\" inleveren. Vaak een andere versie van de opgave die nog ergens op je computer rondslingerde. Zonde van al je werk toch!\n",
    "* Dus, lever **minstens een half uur voor tijd in**. Download dan wat je hebt ingeleverd op Canvas. Geef het een andere naam om verwarring te voorkomen. En draai alle cellen, en bekijk het. Geen syntax fouten? Alle vragen gemaakt? Dan zit het vast wel goed, en hoef je niet in de zenuwen te zitten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21215e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b5e37f3720b0011335209f22450abff",
     "grade": false,
     "grade_id": "cell-aa5556336c90c946",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Applied Machine Learning W2\n",
    "\n",
    "This assignment tests the concepts that have been discussed in this week's lecture, and is divided in several (mostly) independent exercises. \n",
    "\n",
    "1. [Agglomerative Clustering](#agglo)\n",
    "2. [DBSCAN](#dbscan)\n",
    "3. [Evaluation](#elm)\n",
    "4. [Data Normalization](#normalization)\n",
    "5. [PCA](#pca)\n",
    "\n",
    "### TIP\n",
    "The second assignment, which asks you to implement the DBSCAN algorithm, can be a bit challenging, so don't forget about the other exercises! Just start working on another part of the assignment if you get stuck, and come back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0931d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:23.678919Z",
     "iopub.status.busy": "2023-04-11T03:41:23.678186Z",
     "iopub.status.idle": "2023-04-11T03:41:25.765176Z",
     "shell.execute_reply": "2023-04-11T03:41:25.764453Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d94f2dae632e363cb7229b0c8d1b8a",
     "grade": false,
     "grade_id": "imp",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from nose.tools import assert_count_equal, assert_equal\n",
    "from numpy.testing import *\n",
    "from sklearn import datasets\n",
    "# Please do not remove this: \n",
    "np.random.seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679b8fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0853037960cc21f5bab540a250c8717",
     "grade": false,
     "grade_id": "cell-5cfe8bcdbe386cfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"agglo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975df473",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50f017fa3a57a280461b6e4e0cb33c2f",
     "grade": false,
     "grade_id": "agg",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 1 Agglomerative Clustering\n",
    "\n",
    "1. We defined the Ward criterion in class. Now you define from scratch, similar as we did with Ward, *average* and *complete*.\n",
    "2. Create the complete algrithm for bottom up agglomerative clustering from scratch, giving, just like in sklearn, the possibility to choose the linking function. We will help you by having you implement the parts needed as seperate functions.\n",
    "3. Use the Euclidean distance in your functions.\n",
    "3. Try out your code on the iris dataset.\n",
    "4. Compare it to sklearn. I mean the outcomes on the same input. \n",
    "5. Use the technique described in the book to create a dendogram.\n",
    "\n",
    "## Tips\n",
    "* explain what you are doing\n",
    "* test and test and test\n",
    "* refactor, refactor\n",
    "* Be proud. Don't despair if it takes hours to write a few lines of code.\n",
    "    * That is because Python is such a fantastic high level language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a49666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import euclidean\n",
    "# # Start by completing the two functions below, which both takes matrices of size n_samples*n_features\n",
    "# # and return the distance between those clusters according to the linkage functions\n",
    "# def average_linkage(cluster1, cluster2) -> np.float64:\n",
    "#     '''calculates the average distance between cluster 1 and cluster 2'''\n",
    "#     distance = [euclidean(i, j) for i in cluster1 for j in cluster2]\n",
    "#     distance = sum(distance) / len(distance)\n",
    "#     #WRITE YOUR CODE HERE\n",
    "#     return np.float64(distance)\n",
    "\n",
    "# def complete_linkage(cluster1, cluster2) -> np.float64:\n",
    "#     '''calculates maximum distance between cluster 1 and cluster 2'''\n",
    "#     distance = [euclidean(i, j) for i in cluster1 for j in cluster2]\n",
    "#     distance = np.max(distance)\n",
    "    \n",
    "#     return np.float64(distance)\n",
    "\n",
    "# # cluster_1 = np.random.rand(5, 2)\n",
    "# # cluster_2 = np.random.rand(10, 2)\n",
    "# # average_linkage(cluster_1,cluster_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6882ffef",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:25.769202Z",
     "iopub.status.busy": "2023-04-11T03:41:25.768839Z",
     "iopub.status.idle": "2023-04-11T03:41:25.774278Z",
     "shell.execute_reply": "2023-04-11T03:41:25.773736Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1aca492bcbb7271b3ee314ddf60cd8d",
     "grade": false,
     "grade_id": "cell-3ea2cd4a29042123",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Start by completing the two functions below, which both takes matrices of size n_samples*n_features\n",
    "# and return the distance between those clusters according to the linkage functions\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def average_linkage(cluster1, cluster2) -> np.float64:\n",
    "    '''calculates the average distance between cluster 1 and cluster 2'''\n",
    "    distance = np.mean([euclidean(x,y) for x in cluster1 for y in cluster2]) \n",
    "#     distance = np.mean(euclidean_distances(cluster1, cluster2))\n",
    "    return distance\n",
    "\n",
    "def complete_linkage(cluster1, cluster2) -> np.float64:\n",
    "    '''calculates maximum distance between cluster 1 and cluster 2'''\n",
    "    distance = np.max([euclidean(x,y) for x in cluster1 for y in cluster2])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c148bc17",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:25.777351Z",
     "iopub.status.busy": "2023-04-11T03:41:25.777093Z",
     "iopub.status.idle": "2023-04-11T03:41:27.427690Z",
     "shell.execute_reply": "2023-04-11T03:41:27.426885Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e19824f318d83f364907e2553b50600",
     "grade": true,
     "grade_id": "cell-0b2ecbbc446d0351",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that you get the right output\n",
    "cluster_1 = np.random.rand(5, 2)\n",
    "cluster_2 = np.random.rand(10, 2)\n",
    "assert_equal(type(average_linkage(cluster_1, cluster_2)), np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2b9b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e545b4924c30eb441968ea93d1ae1610",
     "grade": false,
     "grade_id": "cell-8ed320830e08b11b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After implementing the two linkage functions, you will implement a single step of the clustering algorithm, which will return the IDs of the clusters in the input that should be merged. As input you will have both the original input data, as well as a dictionary of clusters, where each key is an ID, and each value is a list of points belonging to that cluster. You can use fancy indexing from numpy to get the rows belonging to a cluster easily:\n",
    "\n",
    "```\n",
    "# suppose we have input data X\n",
    "clusters = {0: [0, 1, 2, 3], 4: [4, 6, 7]}\n",
    "# get the vectors of our first cluster\n",
    "cluster_vectors = X[clusters[0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb842b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_step(input_data, cluster_dict, linkage_type: str) -> tuple:\n",
    "    ''' Function that, given an input of vectors such as in the hoorcollege slides\n",
    "    {cluster_id: matrix}\n",
    "    returns the SORTED IDs of the two clusters that should be merged as a list'''\n",
    "    assert linkage_type in ['average', 'complete']\n",
    "    \n",
    "    if linkage_type == 'average':\n",
    "        ids = {(i,j):average_linkage(input_data[cluster_dict[i]], input_data[cluster_dict[j]]) \n",
    "               for x,i in enumerate(cluster_dict) for y,j in enumerate(cluster_dict) if x<y}\n",
    "   \n",
    "    if linkage_type == 'complete':\n",
    "        ids = {(i,j):complete_linkage(input_data[cluster_dict[i]], input_data[cluster_dict[j]]) \n",
    "               for x,i in enumerate(cluster_dict) for y,j in enumerate(cluster_dict) if x<y}\n",
    "    \n",
    "\n",
    "    return min(ids, key= lambda x: ids[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8533a7e3",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:27.431463Z",
     "iopub.status.busy": "2023-04-11T03:41:27.431017Z",
     "iopub.status.idle": "2023-04-11T03:41:27.436069Z",
     "shell.execute_reply": "2023-04-11T03:41:27.435459Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc8027b72773d8030c2a13cb060cdad5",
     "grade": false,
     "grade_id": "cell-a5c39cdf5bd549be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# def clustering_step(input_data, cluster_dict, linkage_type: str) -> tuple:    \n",
    "#     ''' Function that, given an input of vectors such as in the hoorcollege slides\n",
    "#     {cluster_id: matrix}\n",
    "#     returns the SORTED IDs of the two clusters that should be merged as a list'''\n",
    "#     assert linkage_type in ['average', 'complete']\n",
    "#     cluster_ids = []\n",
    "    \n",
    "#     if linkage_type == 'average':\n",
    "#         for x, (key1, value1) in enumerate(cluster_dict.items()):\n",
    "#             for y, (key2, value2) in enumerate(cluster_dict.items()):\n",
    "#                 if x != y:\n",
    "#                     cluster_ids.append(tuple([key1, key2, average_linkage(input_data[value1], input_data[value2])]))\n",
    "#     return tuple(sorted(cluster_ids, key=lambda x: x[2]))[0][:2]\n",
    "\n",
    "# clustering_step(input_array, clusters, linkage_type='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa8109e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:27.438641Z",
     "iopub.status.busy": "2023-04-11T03:41:27.438395Z",
     "iopub.status.idle": "2023-04-11T03:41:27.444726Z",
     "shell.execute_reply": "2023-04-11T03:41:27.444135Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea2aabc53adc8451541df07a61b1bc6f",
     "grade": true,
     "grade_id": "cell-ea15f9c2e6d76953",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "input_array = np.array([\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [3, 3, 3]])\n",
    "clusters = {0: [0], 1: [1], 2: [2]}\n",
    "\n",
    "assert_equal(type(clustering_step(input_array, clusters, linkage_type='average')), tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af2d06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "681e0baf2b35069f7a230df3eb30b4c7",
     "grade": false,
     "grade_id": "cell-f43337b78b4cde61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In our last step, we will implement the complete algorithm for agglomerative clustering, using our previously created `clustering_step` and linkage functions. For the return value, return a list of labels where each label corresponds to the class of the instance. It doesn't matter if these values are not 0/1 as long as items in the same cluster share the same label. \n",
    "\n",
    "**TIP**\n",
    "You can delete key value pairs from dictionaries using the `del dictionary[key]` syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69aed64c",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:27.447642Z",
     "iopub.status.busy": "2023-04-11T03:41:27.447399Z",
     "iopub.status.idle": "2023-04-11T03:41:27.451955Z",
     "shell.execute_reply": "2023-04-11T03:41:27.451457Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b540791a37cc10893ad3097474351441",
     "grade": false,
     "grade_id": "cell-5524b767782337df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# in our version, we will define a number of clusters we want to find\n",
    "# you may also define this in sklearn for your comparison later\n",
    "def agglomerative_clustering(input_points, number_of_clusters: int, linkage_type: str):\n",
    "    assert linkage_type in ['average', 'complete']\n",
    "    \n",
    "    # We start by just having each point in its own cluster\n",
    "    # update this dictionary in the while loop.\n",
    "\n",
    "    clusters = {i: [i] for i in range(len(input_points))}\n",
    "\n",
    "    # Look at the stopping condition, understand why?\n",
    "    while len(clusters) != number_of_clusters:\n",
    "            point_ids = clustering_step(input_points, clusters, linkage_type=linkage_type)\n",
    "            for j in clusters[point_ids[1]]:\n",
    "                clusters[point_ids[0]].append(j)\n",
    "            del clusters[point_ids[1]]\n",
    "    \n",
    "    # Your output should be a dict with as keys the resulting cluster IDs and as values the points belonging to\n",
    "    # that cluster. It does not matter what the ID itself is.\n",
    "    \n",
    "    # create the output array (we will do this for you)\n",
    "    output_labels = np.zeros((input_points.shape[0], 1))\n",
    "    for i, point_ids in enumerate(clusters.values()):\n",
    "        output_labels[point_ids] = i\n",
    "    \n",
    "    return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956980ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:27.454490Z",
     "iopub.status.busy": "2023-04-11T03:41:27.454248Z",
     "iopub.status.idle": "2023-04-11T03:41:27.500130Z",
     "shell.execute_reply": "2023-04-11T03:41:27.499488Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8751bfa61fa74185fa315d6bd3d581d",
     "grade": true,
     "grade_id": "cell-790357a97ba5f07b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Did we actually create the right amount of clusters?\n",
    "samples = np.random.rand(10, 3)\n",
    "labels = agglomerative_clustering(samples, number_of_clusters=4, linkage_type=\"average\")\n",
    "assert_equal(len(np.unique(labels)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33dae5b",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:27.503370Z",
     "iopub.status.busy": "2023-04-11T03:41:27.503112Z",
     "iopub.status.idle": "2023-04-11T03:41:33.463901Z",
     "shell.execute_reply": "2023-04-11T03:41:33.462781Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8de534aefb9196e9822e264da26309f",
     "grade": true,
     "grade_id": "cell-daf2aae5565a776f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True,  True,  True,  True, False, False, False,\n",
       "        False],\n",
       "       [False, False,  True,  True,  True,  True, False, False, False,\n",
       "        False],\n",
       "       [ True,  True, False, False, False, False,  True, False, False,\n",
       "        False],\n",
       "       [ True,  True, False, False, False, False,  True, False, False,\n",
       "        False],\n",
       "       [ True,  True, False, False, False, False,  True, False, False,\n",
       "        False],\n",
       "       [ True,  True, False, False, False, False,  True, False, False,\n",
       "        False],\n",
       "       [False, False,  True,  True,  True,  True, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False,  True,  True,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False,  True,  True,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "         True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare your output with sklearn here\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters = 4, linkage=\"average\").fit(samples)\n",
    "labels1 = clustering.labels_\n",
    "labels2 = agglomerative_clustering(samples, number_of_clusters=4, linkage_type=\"average\")\n",
    "\n",
    "# [label_1 == label_2 for label_1 in labels1 for label_2 in labels2 ]\n",
    "labels1 == labels2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546dfb2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5cfea9e0553e7f8af566f0ffe9681d9",
     "grade": false,
     "grade_id": "cell-74b3f23a427ac8db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you have created the agglomerative algorithm, create the dendogram and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba6938e",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:33.468315Z",
     "iopub.status.busy": "2023-04-11T03:41:33.468003Z",
     "iopub.status.idle": "2023-04-11T03:41:33.471434Z",
     "shell.execute_reply": "2023-04-11T03:41:33.470640Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de38763f9ae732d8a87c894311c776ec",
     "grade": true,
     "grade_id": "cell-70688f9ad0bbf00e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ytdist = samples\n",
    "Z = hierarchy.linkage(ytdist, 'single')\n",
    "plt.figure()\n",
    "dn = hierarchy.dendrogram(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3493b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "777aee9e63136c0c3ab45cd63e20aa21",
     "grade": false,
     "grade_id": "cell-2dc5be82f1c567be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"dbscan\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2150b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72e1a5a2f53e02ebe212a2b6a8d26e0c",
     "grade": false,
     "grade_id": "dbscan",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 2 DBscan\n",
    "\n",
    "In class and in section of the book 3.5.3 the DBScan algorithm was discussed.\n",
    "In this assignment we are going to implement the DBScan metric from scratch, working from first principles and using euclidean distance as the distance metric.\n",
    "\n",
    "**TIPS**\n",
    "- Implementing this from scratch is a challenge, but it should be doable, make sure you really understand/can explain the algorithm before you start coding, this should already help you when programming. Like before, we will also make you implement sub functions first to help you along.\n",
    "- Remember that in the algorithm we include the sample itself in the calculation of the distances and `min_samples`, you don't have to write separate logic to exlcude the point itself from the neighbour calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77de4df",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:33.475356Z",
     "iopub.status.busy": "2023-04-11T03:41:33.475067Z",
     "iopub.status.idle": "2023-04-11T03:41:33.479110Z",
     "shell.execute_reply": "2023-04-11T03:41:33.478508Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e23d3c1b0cd9386dfe53db6bb06e81",
     "grade": false,
     "grade_id": "cell-a6ef76f7add38bfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def get_neighbours(input_data, point_index, eps) -> list:\n",
    "    \"\"\"\n",
    "    Given the entire input data and the index of a specific point, get the indices of all\n",
    "    the neighbor points, i.e. points that are closer than eps away using euclidean distance.\n",
    "    \"\"\"\n",
    "    start = input_data[point_index]\n",
    "    return [x for x in range(len(input_data)) if 0 < euclidean(start,input_data[x]) < eps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128b8ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:33.482070Z",
     "iopub.status.busy": "2023-04-11T03:41:33.481795Z",
     "iopub.status.idle": "2023-04-11T03:41:33.486068Z",
     "shell.execute_reply": "2023-04-11T03:41:33.485528Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "227a28723d3539bad7d768bbcc388970",
     "grade": true,
     "grade_id": "cell-ce6bd3fd8c8a01db",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# check that the output contains integers, which should correspond\n",
    "# to the indices of the neighbours\n",
    "test_data = np.random.rand(5, 3)\n",
    "assert_equal(type(get_neighbours(test_data, 0, eps=1)[0]), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce153d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.random.rand(10, 5)\n",
    "\n",
    "get_neighbours(test_data, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d142bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e02877ab487c8044d18ef7b42f65333",
     "grade": false,
     "grade_id": "cell-ee48d6823dedb0d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After we have implemented the function to get all neighbours of a point for a specific point, we will write a function that, starting from a specific point, finds all neighbour points, and their neighbours points, and so on, until the cluster can no longer be expanded.\n",
    "\n",
    "## TIPS\n",
    " - In this function and the next, we will assume points that have a value of `-1` in the cluster are ure (currently) outliers, and points with a value of `-2` have not yet been examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {i: [i] for i in range(len(test_data))}\n",
    "EPS = 0.5\n",
    "MIN_SAMPLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd1662",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:33.489243Z",
     "iopub.status.busy": "2023-04-11T03:41:33.488964Z",
     "iopub.status.idle": "2023-04-11T03:41:33.493483Z",
     "shell.execute_reply": "2023-04-11T03:41:33.492903Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cd074839ad2dcc59345b30cb13b1a65",
     "grade": false,
     "grade_id": "cell-530e8aff991748e1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def expand(input_data, clusters, point, class_label, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Given the input data, the array with the current clusters assignments,\n",
    "    and the ID for the class, return all the points in the class, iteratively\n",
    "    going over neighbours, neigbours of neighbours etc.\n",
    "    \"\"\"\n",
    "    neighbours = get_neighbours(input_data, point, eps)\n",
    "    clusters[point] = class_label\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(neighbours):\n",
    "        \n",
    "        i += 1\n",
    "    return clusters\n",
    "expand(test_data, clusters, 1, [-1,-2], EPS, MIN_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953753e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f67771e1332793ac078418a69f180f3",
     "grade": false,
     "grade_id": "cell-0d1aed0778a65657",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below we will put everything we made so far together and create the complete DBSCAN algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d396d2",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:33.496859Z",
     "iopub.status.busy": "2023-04-11T03:41:33.496580Z",
     "iopub.status.idle": "2023-04-11T03:41:33.501123Z",
     "shell.execute_reply": "2023-04-11T03:41:33.500450Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "622e048dd59e3930554d932596c165b7",
     "grade": true,
     "grade_id": "dbscana",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dbscan(input_data, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Implements the complete dbscan algorithm using the helper functions we wrote before.\n",
    "    Takes as input the complete input matrix, the maximum distance neighbours can be apart, eps,\n",
    "    and the minimal number of samples that are neighbours from a point to let it be considered\n",
    "    a core point. Returns an array with for each item the class label of that item, where -1\n",
    "    indicates that the item is an outlier.\n",
    "    \"\"\"\n",
    "    current_class = 0\n",
    "    # start with empty cluster assignments\n",
    "    clusters = [-2 for _ in range(input_data.shape[0])]\n",
    "    \n",
    "    for i in range(0, input_data.shape[0]):\n",
    "        #WRITE YOUR CODE HERE\n",
    "        \n",
    "    return np.array(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cbff0",
   "metadata": {},
   "source": [
    "Compare the output of your algorithm to the output of sklearns `DBSCAN` in the cell below, explaing your tests in the comments of the cell\n",
    "\n",
    "### TIP\n",
    "\n",
    "- Have a look at `make_blobs` from the sklearn.datasets module, this is a nice function with which you can easily\n",
    "create some sample clusters to test your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64da00",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:33.504359Z",
     "iopub.status.busy": "2023-04-11T03:41:33.504082Z",
     "iopub.status.idle": "2023-04-11T03:41:36.439870Z",
     "shell.execute_reply": "2023-04-11T03:41:36.438720Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23913fd67d8375bfb9be9f5720e28d96",
     "grade": true,
     "grade_id": "cell-6e4d46c5a1c0a696",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compare to sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "sample_data, eps, min_sample = np.random.rand(100, 3), 1, 2\n",
    "clustering = DBSCAN(eps=eps, min_samples=min_sample).fit(sample_data)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b8426",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30a039da03152fc6b38297ca1d9e73a2",
     "grade": false,
     "grade_id": "cell-8e15e345ff215048",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"elm\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0938df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9906a1b86c2a6d5530644c97cb372ee",
     "grade": false,
     "grade_id": "elm",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 3 Evaluation\n",
    "\n",
    "We can evaluate a cluster algorithm against a gold standard labelling. The tricky part then is which cluster to assign to which label. Think of Iris, and you have clustered it into 3 groups. Then which group do you want to assign to which species?\n",
    "\n",
    "A smart idea called BCubed, and later improved to [ELM](https://dl.acm.org/doi/abs/10.1145/3539813.3545121) can be used to evaluate the quality of a clustering.\n",
    "\n",
    "* implement ELM\n",
    "* For this exercise, implement the F1 score, where you calculate F1 for each element and then average it over all elements\n",
    "* test it with a number of clusterings you made of the IRIs set and another own found set with gold standard.\n",
    " - If you managed to implement your own version DBSCAN you can use these to make clusters of the IRIS dataset, otherwise just use DBSCAN from `sklearn`.\n",
    "* Experiment with normalization, and see the effect.\n",
    "* You are not training anything, so you simply cluster the full set and use the same set as test set.\n",
    "\n",
    "* Next to the code, you write a small report in a markdown cell on your findings.\n",
    "\n",
    "**TIPS**:\n",
    " - The ELM algorithm operates on points, and compares the cluster a point is in in the\n",
    " gold standard with a cluster a point is in in the prediction. \n",
    " - Using sets is useful!\n",
    " - Start by creating a nested dict where each key is the index of a point, and each value is a dict with the indices of its neighbours, exluding the element itself. Do this for buth the gold standard and the predicted cluster.\n",
    "\n",
    "\n",
    "### Test dbscan's parameters\n",
    "\n",
    "* Using our ELM algorithm, try several options for the DBSCAN algorithm and report which values worked best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c26af1",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:36.444313Z",
     "iopub.status.busy": "2023-04-11T03:41:36.444020Z",
     "iopub.status.idle": "2023-04-11T03:41:36.449687Z",
     "shell.execute_reply": "2023-04-11T03:41:36.449130Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c71dd99b82fa93fb7d4750b9487551e",
     "grade": false,
     "grade_id": "elm1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your input will be an array of point labels, as you would get as the output of an sklearn\n",
    "# model.\n",
    "def ELM(gold_standard_labels, predicted_labels):\n",
    "    score = []\n",
    "    num_points = len(gold_standard_labels)\n",
    "    #WRITE YOUR CODE HERE\n",
    "      \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc5cca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:36.452582Z",
     "iopub.status.busy": "2023-04-11T03:41:36.452209Z",
     "iopub.status.idle": "2023-04-11T03:41:36.456304Z",
     "shell.execute_reply": "2023-04-11T03:41:36.455771Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bee2f3df6f8d47fa5f65ce004ef3d8bb",
     "grade": true,
     "grade_id": "cell-dc7be1edda2e5f0e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(ELM([0, 1, 2, 2], [0, 0, 0, 0])), np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c8f30",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:36.458966Z",
     "iopub.status.busy": "2023-04-11T03:41:36.458734Z",
     "iopub.status.idle": "2023-04-11T03:41:43.434751Z",
     "shell.execute_reply": "2023-04-11T03:41:43.433946Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "533ddd45c7daa245931c56267af09569",
     "grade": true,
     "grade_id": "elm2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Try different variations of DBSCAN, and evaluate using the ELM score we just defined.\n",
    "\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd6789",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b1060268a1f939dd9c12e02dd0b009b",
     "grade": false,
     "grade_id": "cell-dd21fc6122e9947d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"normalization\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6628758",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "802ad376b9b2f3b2ab284cc5fa698281",
     "grade": false,
     "grade_id": "pp",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Preprocessing and scaling section 3.2\n",
    "\n",
    "\n",
    "1. Download iris data, and Z-transform it using your own code.\n",
    "    2. Give a convincing check/test(s) that it worked.\n",
    "2. Similary for the `min-max` scalar and sklearn's `normalizer`.\n",
    "    * Of course you also describe in good English and with a math formula what this scaler is doing.\n",
    "3. Figure out how sklearn's *RobustScaler* works, and implement it using your own code. You can use numpy to get the 1st and 3rd quartile.\n",
    "4. Test all your code on iris and at least one other dataset.\n",
    "    * Make your own tests which really test the math.\n",
    "    * Run the same ytransformation in sklearn and use something like `assert_array_almost_equal` to test that you are close enough to sklearn's.\n",
    "\n",
    "Answer both in code and in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c23ea87b",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:43.439042Z",
     "iopub.status.busy": "2023-04-11T03:41:43.438743Z",
     "iopub.status.idle": "2023-04-11T03:41:43.451750Z",
     "shell.execute_reply": "2023-04-11T03:41:43.450958Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e16f6db51c5db571544e800f17c02f2d",
     "grade": false,
     "grade_id": "pp1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# get the iris dataset\n",
    "iris_X, iris_y = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "def z_transform(input_data):    \n",
    "    output_data = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "    return np.vstack(output_data)\n",
    "\n",
    "def my_normalize(input_data):\n",
    "    norms = np.reshape(np.linalg.norm(input_data, axis=1), (input_data.shape[0],1))\n",
    "    output_data = input_data / norms\n",
    "    return np.vstack(output_data)    \n",
    "\n",
    "def min_max(input_data, min_val, max_val):   \n",
    "#     standard = (input_data - min_val) / (max_val - min_val)\n",
    "#     output_data = standard * (standard.max() - standard.min()) + standard.min()\n",
    "    output_data = (input_data - min_val) / (max_val - min_val)\n",
    "    return np.vstack(output_data)\n",
    "\n",
    "def robust_scaler(input_data):\n",
    "    IQR = np.percentile(input_data, 75) - np.percentile(input_data, 25)\n",
    "    \n",
    "#     output_data = (input_data - np.percentile(input_data, 25)) / (np.percentile(input_data, 75) - np.percentile(input_data, 25))\n",
    "    \n",
    "#     Replace IQR values of 0 with a small value to avoid division by zero\n",
    "    if IQR == 0: IQR = 1e-12\n",
    "        \n",
    "    output_data = (input_data - np.mean(input_data)) / IQR\n",
    "    \n",
    "    return np.vstack(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102112c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f1beeaf8e13a82f5b6f50fa09ebd8c1",
     "grade": false,
     "grade_id": "cell-f493f8f905f7c1b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Briefly describe for each scaler what they do (and give the formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304c872",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "938200fa7dc00cb3f56b26859f80615b",
     "grade": true,
     "grade_id": "cell-b707fe459099e51b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "> **Z-transform or standard Scaler:**\n",
    "> $$ \\frac{x - mean(x)}{sd(x)} $$\n",
    "> the Standard Scaler assumes that the data within each feature follows a normal distribution, and it transforms the data to have a mean of 0 and a standard deviation of 1 by centering the distribution around 0 and removing the mean.\n",
    "\n",
    "\n",
    "\n",
    "> **Normalize:**\n",
    "> $$ \\frac{x - x_{min}}{x_{min} - x_{min}} $$\n",
    "> Samples that contain at least one non-zero component are independently rescaled so that their norm, or magnitude, is equal to one.\n",
    "\n",
    "> **Min Max:**\n",
    "> $$ \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "> The min-max scaling technique adjusts the values of each feature to fall within a specified range.\n",
    "\n",
    "\n",
    "> **Robust:**\n",
    "> $$ \\frac{x - Q_1(x)}{Q_3(x) - Q_1(x)} $$\n",
    "> The Robust Scaler is a feature scaling method that is resilient to outliers in the data. It is similar to the MinMax Scaler, but instead of using the minimum and maximum values, it employs the interquartile range (IQR). The median and scales of the data are adjusted based on the quantile range, which makes it robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb96e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d1413a56149c2decce443ee4feaf225",
     "grade": false,
     "grade_id": "cell-961f5e0a66bd6859",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For each of the functions you implemented above, write tests in the cell below to make sure your scaling is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f13ab17e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e82fbed452ad2d759d09f86f3e2393ea",
     "grade": true,
     "grade_id": "cell-f92d1ebc3600c4b4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-fransform accuracy: 100%\n",
      "Normalize accuracy: 84%\n",
      "Min Max accuracy: 49%\n",
      "Robust accuracy: 0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, RobustScaler\n",
    "\n",
    "# z-transform\n",
    "print(\"Z-fransform accuracy: {}%\".format(round(\n",
    "    (z_transform(iris_X) == StandardScaler().fit_transform(iris_X)).mean() * 100),2))\n",
    "\n",
    "# normalize\n",
    "print(\"Normalize accuracy: {}%\".format(round(\n",
    "    (my_normalize(iris_X) == Normalizer().fit_transform(iris_X)).mean() * 100),2))\n",
    "\n",
    "#min max\n",
    "print(\"Min Max accuracy: {}%\".format(round(\n",
    "    (min_max(iris_X, iris_X.min(axis=0), iris_X.max(axis=0)) == MinMaxScaler().fit_transform(iris_X)).mean() * 100),2))\n",
    "\n",
    "# robust\n",
    "print(\"Robust accuracy: {}%\".format(round(\n",
    "    (robust_scaler(iris_X) == RobustScaler().fit_transform(iris_X)).mean() * 100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281c7d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b4670721462ddb2b2f4631ac5e274a7",
     "grade": false,
     "grade_id": "cell-06b3e7850304b90f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"pca\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220deb8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f0dbeb760f236efdcc65798aa2711d4",
     "grade": false,
     "grade_id": "pca",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# PCA\n",
    "\n",
    "### inspect\n",
    "* get the sklearn digit dataset.\n",
    "* transform it to 2 dimensions using PCA, you are allowed to use the `sklearn` function for this exercise.\n",
    "* plot it in 2 dimensions using seaborn and use the `hue` parameter to see how much the digits are separated.\n",
    "* Does it make sense? Do you see similar \"confusions/overlap\" as seen before in the book between certain digits?\n",
    " - Play around with the color palete to ensure that the different classes have clearly different colours.\n",
    "\n",
    "\n",
    "### cluster\n",
    "\n",
    "Run K-means on this reduced data (2 instead of 64 dimensions!), also run K-means on the original data. Then compare the two using the ELM-metric. For this experiment you are allowed to use the K means algorithm from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd3de2",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:43.456250Z",
     "iopub.status.busy": "2023-04-11T03:41:43.455948Z",
     "iopub.status.idle": "2023-04-11T03:41:43.472110Z",
     "shell.execute_reply": "2023-04-11T03:41:43.471462Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b23f55f7df5b6de8ef18ffe0be5d6338",
     "grade": false,
     "grade_id": "pca1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Load in the digit dataset\n",
    "digits_X, digits_y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "# You are allowed to fix the pca to always return two dimensions\n",
    "def perform_pca(input_vectors):\n",
    "    output_vectors = None\n",
    "    \n",
    "    #WRITE YOUR CODE HERE\n",
    "\n",
    "    return output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082aea3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:43.475432Z",
     "iopub.status.busy": "2023-04-11T03:41:43.475103Z",
     "iopub.status.idle": "2023-04-11T03:41:43.503629Z",
     "shell.execute_reply": "2023-04-11T03:41:43.502681Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a457abf9743a62a070d863052850b21e",
     "grade": true,
     "grade_id": "cell-dbb42e9722e026d4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(perform_pca(digits_X).shape[-1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545b426",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:43.508265Z",
     "iopub.status.busy": "2023-04-11T03:41:43.507952Z",
     "iopub.status.idle": "2023-04-11T03:41:44.110047Z",
     "shell.execute_reply": "2023-04-11T03:41:44.109177Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eeab35b70dfe24810aba71074eaf45ef",
     "grade": false,
     "grade_id": "cell-b98baba6d68fee4b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement your plotting here\n",
    "import matplotlib.pyplot as plt\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd759f79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7bb11ba88acbf081f7adf8e54970a9c",
     "grade": false,
     "grade_id": "cell-1b2f2dab6930142d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does your plot make sense? Do you see similar overlaps as in the book?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23730aba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02eca7d0c0c06674663559214eb5149a",
     "grade": true,
     "grade_id": "pca2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d91e1",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-04-11T03:41:44.114596Z",
     "iopub.status.busy": "2023-04-11T03:41:44.114291Z",
     "iopub.status.idle": "2023-04-11T03:41:44.406456Z",
     "shell.execute_reply": "2023-04-11T03:41:44.405771Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bd71c3cd0a849b2a2f6b3fc39813d06",
     "grade": true,
     "grade_id": "cell-7438942e6e2adfa8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compare K means on both the reduced and unreduced data and evaluation using ELM.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc04e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
