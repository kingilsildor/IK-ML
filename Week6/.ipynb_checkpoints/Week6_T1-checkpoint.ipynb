{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2bc76",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Week6_T1.ipynb to slides\n",
      "[NbConvertApp] Writing 592395 bytes to Week6_T1.slides.html\n",
      "[NbConvertApp] Redirecting reveal.js requests to https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0\n",
      "Serving your slides at http://127.0.0.1:8000/Week6_T1.slides.html\n",
      "Use Control-C to stop this server\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert Week6_T1.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e404154c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341927d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week6 Supervised machine learning: classification\n",
    "\n",
    "* 3 models\n",
    "    * linear\n",
    "        * logistic regression\n",
    "        * SVM\n",
    "    * non parametric\n",
    "        * good old KNN of course\n",
    "        * decision trees and ensembles of them: random forests\n",
    "* Topics\n",
    "    * sigmoid/logistic function\n",
    "    * gradient descent and the cost function\n",
    "    * regularization\n",
    "    * binary vs multiclass classification\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d28d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stof \n",
    "\n",
    "* Section 2.3.3 from page 58 till the end, sections 2.3.5 and 2.3.6 from the book.\n",
    "* [DS handbook on SVM](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb)\n",
    "* [DS handbook on decision trees and random forest](https://github.com/jakevdp/PythonDataScienceHandbook/blob/8a34a4f653bdbdc01415a94dc20d4e9b97438965/notebooks/05.08-Random-Forests.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be490ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Be careful\n",
    "\n",
    "* Lecture will be chaotic, using \n",
    "    * slides from Prof Kanoulas\n",
    "    * notebooks from the Data Science Handbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fa979",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification\n",
    "\n",
    "## Task\n",
    "\n",
    "* What was it again?\n",
    "\n",
    "\n",
    "## Very simple, possibly simplest, model\n",
    "\n",
    "* What was that again again?\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "* What were the measures again?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca216ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification\n",
    "\n",
    "## Task\n",
    "\n",
    "* Given the description of an item as values on a set of features, **predict its class**, \n",
    "    * the value on a categorical variable\n",
    "    * either true/false or multiclass (eg. vmbo, havo, vwo)\n",
    "\n",
    "\n",
    "## Very simple model\n",
    "\n",
    "* KNN, zoek de k dischtsbijijnde al geziene voorbeelden, en kijk welke klasse het meest voorkomt, en voorspel dan die.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "* Gebaseerd op de *confusion matrix*: precision, recall, en hun harmonisch gemiddelde, F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5410b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classificatie: soorten modellen\n",
    "\n",
    "* Niet parametrisch\n",
    "    * Maken geen aanname op de vorm van de data\n",
    "    * KNN\n",
    "    * beslisbomen\n",
    "* linear\n",
    "    * \"een rechte lijn\" (hyperplane) kan de klassen van elkaar scheiden\n",
    "        * nou ja, recht... denk aan PolyNomial Regression\n",
    "    * Naive Bayes, logistic regression, SVM\n",
    "* niet linear\n",
    "    * Neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b669a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression in a nutshell\n",
    "\n",
    "* Tweak familiar linear regression such that it yields an output between 0 and 1.\n",
    "* Interpret that as the **probability** that an item belongs to the True class.\n",
    "* Set a boundary, usually at .5:\n",
    "    * if $P(x)\\leq  .5$ then $x$ NOT in the true class, \n",
    "    * else, it is.\n",
    "    \n",
    "### De sigmoid of de  [logistic function](https://en.wikipedia.org/wiki/Logistic_function)\n",
    "\n",
    "* domein is de hele $\\mathbf{R}$, de hele x-as.\n",
    "* bereik is $(0,1)$, dus wordt nooit 0 en nooit 1.\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1920px-Logistic-curve.svg.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e5cba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Slides Kanoulas\n",
    "\n",
    "<Week6/LogisticRegression.pdf>\n",
    "\n",
    "### Additions\n",
    "\n",
    "* Update rule for gradient descent for linear and logistic regression is identical!\n",
    "* Even though the loss function is very different! \n",
    "    * But actually this is **by design** of the log loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0acb85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Partial derivative  for logistic regression\n",
    "\n",
    "* We use the update rule for gradient descent.\n",
    "* Is,   by the choice of the costfunction, **exactly the same as for linear regression**\n",
    "\n",
    "$$\\frac{\\partial(J(\\theta))}{\\partial(\\theta)}= \\frac{\\sum_{i=1}^{i=n} x_i^j\\cdot e_i}{n}$$\n",
    "\n",
    "\n",
    "* See [this blogpost](https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d) for the (very lengthy) derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55504b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiclass classification\n",
    "\n",
    "<Week6/MulticlassClassification.pdf> starting at slide/page 20.\n",
    "\n",
    "### take away\n",
    "\n",
    "* With $N$ classes, create for each class a **binary classifier** class versus rest.\n",
    "* At test time: run the  classifiers for each class \n",
    "    * output: a probability that the test-item belongs to that class.\n",
    "    * take the *argmax*: assign that class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771a3ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM\n",
    "\n",
    "* Instead of the slides of Kanoulas, we use the **updated version** of the Data Science Handbook notebook on SVM.\n",
    "\n",
    "* <http://localhost:8888/notebooks/onderwijs/teaching/Previous/DataScience/Teacher/PythonDataScienceHandbook/notebooks/05.07-Support-Vector-Machines.ipynb>\n",
    "\n",
    "### take away\n",
    "\n",
    "* SVM solves the problem which decision boundary to choose when doing logisctic regression.\n",
    "    * the one with the largest **error margin** \n",
    "* the training examples  closest to  the decision boundary are the **support vectors**\n",
    "    * the model only depends on them.\n",
    "* **++**\n",
    "    * usually better performance than logistic regression\n",
    "* **--**\n",
    "    * more difficult optimization function: training needs more time\n",
    "    * also testing can be slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f1946",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM: the problem it solves\n",
    "\n",
    "* Problem: where to draw the decision boundary?\n",
    "\n",
    "![](img/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9e3bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# where?\n",
    "\n",
    "* Consider new point \"red cross\"\n",
    "    * Which class it is in depends on the boundary chosen\n",
    "![](img/svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4786d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What would intuitively be the best boundary?\n",
    "\n",
    "\n",
    "* Can you make that precise?\n",
    "* On what does it depend?\n",
    "\n",
    "![](img/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0edbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Large \"empty margin\" around the boundary \n",
    "\n",
    "* the larger the better\n",
    "* depends on the \"difficult\" training points only\n",
    "    * those close to the boundary\n",
    "| | |\n",
    "|:-:|:-:|\n",
    "|![](img/svm3.png)| ![](img/svm4.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4c4b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Indeed\n",
    "\n",
    "* adding \"easy\" (= far from the boundary) training points does not change the classifier\n",
    "* quite intuitive\n",
    "* \"you learn from the edges cases\"\n",
    "\n",
    "![](img/svm5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1bdc8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees and random forest\n",
    "\n",
    "* <http://localhost:8888/notebooks/onderwijs/teaching/Previous/DataScience/Teacher/PythonDataScienceHandbook/notebooks/05.08-Random-Forests.ipynb>\n",
    "\n",
    "### Take away\n",
    "\n",
    "* Niet parametrisch: geen enkele aanname nodig over de data.\n",
    "* Beslisboom is heel intuitief, en ook makkelijk te begrijpen.\n",
    "* **Maar** overfit heel snel, vooral als je wat dieper gaat.\n",
    "* Oplossing, maak een \"bos\" in plaats van een boom\n",
    "    * train heel veel overgefitte beslisbomen op samples van de train data\n",
    "    * maak daarvan een \"ensemble\"\n",
    "    * Op test-tijd, gebruik alle bomen in het ensemble, turf de uitkomsten\n",
    "        * de meest geturfde klasse, die wordt het.\n",
    "        * zelfs een probabalistische interpretatie: $P(C|x)=$ aantal beslisbomen in het ensemble  dat $x$ als $C$ classificeerd,  gedeeld door totaal aantal beslisbomen in het ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabbc5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap up: classificatie\n",
    "\n",
    "* 3 models\n",
    "    * linear\n",
    "        * logistic regression\n",
    "        * SVM\n",
    "    * non parametric\n",
    "        * good old KNN of course\n",
    "        * decision trees and ensembles of them: random forests\n",
    "* Allemaal heel krachtig,\n",
    "    * kneepjes zitten in finetuning en overfitting voorkomen\n",
    "    * Random Forest heel makkelijk, vrijwel niks te trainen, heel sterk.\n",
    "        * XGBoost is een geoptimaliseerde random forest classifier.\n",
    "        \n",
    "### Volgende week: nog weer wat krachtiger: neurale netwerken"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
